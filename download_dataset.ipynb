{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c623a9",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "67d6d373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:08:36.025851Z",
     "start_time": "2025-01-06T18:08:28.398456Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "96cba08b",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06b4c3",
   "metadata": {},
   "source": [
    "Il dataset contiene 100k training images divise per 200 classi. Ciò che vedrete, scaricando il dataset, saranno 3 folders (train, test e validation) e due file di testo che contengono informazioni generiche sui nomi delle cartelle e sul tipo di classe che rappresentano (persone, entità, ecc.). La train folder contiene altre 200 cartelle, corrispondenti alle 200 classi, che contengono 500 immagini ognuna. I file nomecartella_boxes.txt che vedete contengono informazioni sulle bounding box presenti in ciascuna immagine (coordinate x e y del top-left corner, width e height). Validation e test contengono 10k immagini ognuna e sono shufflate randomicamente.\n",
    "\n",
    "Purtroppo il dataset non si trova in nessuna libreria python, quindi va necessariamente scaricato sul vostro pc. Vi lascio il link: http://cs231n.stanford.edu/tiny-imagenet-200.zip \n",
    "\n",
    "Non vi preoccupate se vi dice che il sito non è sicuro, voi scaricatelo comunque. Per ora il computer non mi è esploso quindi credo vada bene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0828f7",
   "metadata": {},
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "id": "26688701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:26:32.604849Z",
     "start_time": "2025-01-06T17:03:31.868726Z"
    }
   },
   "source": [
    "# Cambiate il path a seconda di dove mettete il dataset\n",
    "# La r non è un errore: serve ad evitare l'unicode error\n",
    "dataset_dir = r\"C:\\Users\\Ari\\Desktop\\Uni\\CV\\Project\\tiny-imagenet-200\\tiny-imagenet-200\\train\"\n",
    "train_data = []\n",
    "\n",
    "for folder_name in os.listdir(dataset_dir): # entra semplicemente in ciascuna delle 200 folder\n",
    "    folder_path = os.path.join(dataset_dir, folder_name)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        images_folder_path = os.path.join(folder_path, 'images')  \n",
    "        # in ciascuna folder, c'è una cartella images e un file .txt che mi stava portando casini: con questa riga di codice\n",
    "        # vado a considerare solo le cartelle images e ad ignorare il file di testo\n",
    "        \n",
    "        if os.path.isdir(images_folder_path): \n",
    "            # ora sono dentro la cartella images (iterata sempre sulle 200 folders)\n",
    "            for img_name in os.listdir(images_folder_path):\n",
    "                img_path = os.path.join(images_folder_path, img_name)\n",
    "                \n",
    "                if os.path.isfile(img_path) and (img_path.endswith('.JPEG')):\n",
    "                    try:\n",
    "                        img = cv2.imread(img_path)\n",
    "\n",
    "                        if img is not None:  \n",
    "                            train_data.append(img)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "# Sto usando numpy invece di torch ma credo vada bene comunque: ho controllato e ciascuna immagine è 64x64x3 quindi dovrebbe \n",
    "# andar bene. Voi controllate per sicurezza con le righe che commento sotto\n",
    "train_data = np.array(train_data)\n",
    "# train_data[:1].shape (dovrebbe uscire una roba del tipo (1, 64, 64, 3) perchè le immagini sono RGB e 64x64 come dice il sito)\n",
    "# len(train_data) # dovrebbe uscire 100k perchè, come detto prima, sono 100k immagini"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "2daff369",
   "metadata": {},
   "source": [
    "Questo pezzo di codice, su jupyter anaconda, impiega circa 10 minuti. Per evitare di dover runnare il codice di sopra ogni volta, salvo l'array su un path a mia scelta e all'occorrenza lo carico:"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb2555d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T17:28:48.262848Z",
     "start_time": "2025-01-06T17:28:46.048268Z"
    }
   },
   "source": "np.save(r\"train_data.npy\", train_data)",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "1e984d97",
   "metadata": {},
   "source": [
    "#### CELLA DA RUNNARE PER IL TRAIN_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c070980",
   "metadata": {},
   "outputs": [],
   "source": "train_data = np.load(r\"train_data.npy\")"
  },
  {
   "cell_type": "markdown",
   "id": "40004d48",
   "metadata": {},
   "source": [
    "#### Validation e test"
   ]
  },
  {
   "cell_type": "code",
   "id": "c12c1676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T18:11:49.844369Z",
     "start_time": "2025-01-06T18:10:24.898105Z"
    }
   },
   "source": [
    "validation_dir = r\"C:\\Users\\Ari\\Desktop\\Uni\\CV\\Project\\tiny-imagenet-200\\tiny-imagenet-200\\val\\images\"\n",
    "test_dir = r\"C:\\Users\\Ari\\Desktop\\Uni\\CV\\Project\\tiny-imagenet-200\\tiny-imagenet-200\\test\\images\"\n",
    "# state attente ad inserire \\images alla fine se no vi considera pure il file di testo\n",
    "\n",
    "validation_data = []\n",
    "test_data = []\n",
    "\n",
    "def load_images_from_directory(directory):\n",
    "    data = []\n",
    "    for img_name in os.listdir(directory):\n",
    "        img_path = os.path.join(directory, img_name)\n",
    "        if os.path.isfile(img_path) and (img_path.endswith('.JPEG')):\n",
    "            try:\n",
    "\n",
    "                img = cv2.imread(img_path)\n",
    "\n",
    "                if img is not None:  \n",
    "                    data.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "validation_data = load_images_from_directory(validation_dir)\n",
    "test_data = load_images_from_directory(test_dir)\n",
    "\n",
    "\n",
    "validation_data = np.array(validation_data)\n",
    "np.save(r\"val_data.npy\", validation_data)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "np.save(r\"test_data.npy\", test_data)\n",
    "\n",
    "print(f\"Loaded {len(validation_data)} validation images.\")\n",
    "print(f\"Loaded {len(test_data)} test images.\")\n",
    "# se tutto è andato bene, dovrebbe darvi 10k per entrambi \n",
    "print(validation_data[:1].shape, test_data[:1].shape) # dovrebbe risultare come nella cella sopra"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 validation images.\n",
      "Loaded 10000 test images.\n",
      "(1, 64, 64, 3) (1, 64, 64, 3)\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
